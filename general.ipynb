{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\bahaa\\.cache\\kagglehub\\datasets\\iamsouravbanerjee\\animal-image-dataset-90-different-animals\\versions\\5\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"iamsouravbanerjee/animal-image-dataset-90-different-animals\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation, RandomResizedCrop,CenterCrop\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,StepLR\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from accelerate import Accelerator\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist ##these are for multiple gpu\n",
    "accelerator = Accelerator()\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    # Randomly flip images horizontally\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Resize(128),\n",
    "    CenterCrop(128),\n",
    "    # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "animal_dataset = datasets.ImageFolder(root=path,  # Specify the root directory of the dataset\n",
    "                               transform=transform)  # Apply the defined transformations to the dataset\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(animal_dataset, 32, shuffle=True, num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import random\n",
    "class GetPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, patch_size=16):\n",
    "        super(GetPatchEmbeddings, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def divide_into_patches(self,image , patch_size=16):\n",
    "        \"\"\"\n",
    "        Divides an image into non-overlapping patches.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): Input image of shape (C, H, W).\n",
    "            patch_size (int): Size of each square patch (patch_size x patch_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of patches of shape (num_patches, C, patch_size, patch_size).\n",
    "        \"\"\"\n",
    "        image = image.to(device)\n",
    "        B,C, H, W = image.shape\n",
    "        NUM_PATCHES = int(H*W/(patch_size**2))\n",
    "        patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size).to(device)\n",
    "        patches = patches.permute(0,2, 3, 1, 4, 5).reshape(B, NUM_PATCHES,C, patch_size, patch_size).to(device)\n",
    "        return patches\n",
    "\n",
    "    def sample_context_blocks(self,patches, m=49, image_size=(128,128), patch_size=16 ):\n",
    "        \"\"\"\n",
    "        Randomly selects m continuous patches forming a square block (LxL).\n",
    "\n",
    "        Args:\n",
    "            patches (torch.Tensor): Tensor of patches of shape (num_patches, C, patch_size, patch_size).\n",
    "            image_size (tuple): Original image size as (H, W).\n",
    "            patch_size (int): Size of each patch (patch_size x patch_size).\n",
    "            m (int): Number of continuous patches to select.\n",
    "\n",
    "        Returns:\n",
    "            list: Indices of selected patches.\n",
    "            torch.Tensor: Selected patches of shape (m, C, patch_size, patch_size).\n",
    "        \"\"\"\n",
    "        H, W = image_size\n",
    "        n_patches_h = H // patch_size\n",
    "        n_patches_w = W // patch_size\n",
    "        patches = patches.to(device)\n",
    "        # Compute side length of the square block (LxL = m)\n",
    "        side_length = int(m ** 0.5)\n",
    "        assert side_length ** 2 == m, \"m must be a perfect square to form a square block.\"\n",
    "\n",
    "        # Randomly select a top-left corner for the square block\n",
    "        max_row = n_patches_h - side_length\n",
    "        max_col = n_patches_w - side_length\n",
    "        start_row = random.randint(0, max_row)\n",
    "        start_col = random.randint(0, max_col)\n",
    "\n",
    "        # Collect indices of the patches in the square block\n",
    "        context_indices = []\n",
    "        for _ in range(patches.shape[0]):\n",
    "            context_indices.append([\n",
    "            (start_row + i) * n_patches_w + (start_col + j)\n",
    "            for i in range(side_length)\n",
    "            for j in range(side_length)\n",
    "        ])\n",
    "\n",
    "        masked_patches = patches.clone().to(device)\n",
    "        for j in range(patches.shape[0]):\n",
    "            for i in range(len(patches[j])):\n",
    "                if i not in context_indices[j]:\n",
    "                    masked_patches[j][i] = torch.zeros_like(masked_patches[j][i])  # Mask unselected patches to zero\n",
    "\n",
    "        return masked_patches, context_indices\n",
    "\n",
    "    def sample_target_blocks(self,patches, m =9,  image_size=(128,128), patch_size=16):\n",
    "        \"\"\"\n",
    "        Randomly selects m continuous patches forming a square block (LxL).\n",
    "\n",
    "        Args:\n",
    "            patches (torch.Tensor): Tensor of patches of shape (num_patches, C, patch_size, patch_size).\n",
    "            image_size (tuple): Original image size as (H, W).\n",
    "            patch_size (int): Size of each patch (patch_size x patch_size).\n",
    "            m (int): Number of continuous patches to select.\n",
    "\n",
    "        Returns:\n",
    "            list: Indices of selected patches.\n",
    "            torch.Tensor: Selected patches of shape (m, C, patch_size, patch_size).\n",
    "        \"\"\"\n",
    "        patches = patches.to(device)\n",
    "        H, W = image_size\n",
    "        n_patches_h = H // patch_size\n",
    "        n_patches_w = W // patch_size\n",
    "        ar = (1.5 - 0.75) * torch.rand((1)) + 0.75 # aspect ratio\n",
    "\n",
    "        # Compute side length of the target block\n",
    "        side_length_h = int(ar*(m)**(0.5))\n",
    "        side_length_w = int(m/side_length_h)\n",
    "\n",
    "        # Randomly select a top-left corner for the square block\n",
    "        max_row = n_patches_h - side_length_h\n",
    "        max_col = n_patches_w - side_length_w\n",
    "        start_row = random.randint(0, max_row)\n",
    "        start_col = random.randint(0, max_col)\n",
    "\n",
    "        # Collect indices of the patches in the square block\n",
    "        target_indices = []\n",
    "        for _ in range(patches.shape[0]):\n",
    "            target_indices.append([\n",
    "            (start_row + i) * n_patches_w + (start_col + j)\n",
    "            for i in range(side_length_h)\n",
    "            for j in range(side_length_w)\n",
    "        ])\n",
    "\n",
    "        masked_patches = patches.clone().to(device)\n",
    "        for j in range(patches.shape[0]):\n",
    "            for i in range(len(patches[j])):\n",
    "                if i not in target_indices[j]:\n",
    "                    masked_patches[j][i] = torch.zeros_like(masked_patches[j][i])  # Mask unselected patches to zero\n",
    "\n",
    "        return masked_patches, target_indices\n",
    "\n",
    "    def remove_overlaps(self,context_blocks, context_indices, NUM_TARGETS=4):\n",
    "\n",
    "        TARGET_INDICES = []\n",
    "        for i in  range(NUM_TARGETS):\n",
    "            \n",
    "            target,indices = context_blocks.to(device),context_indices\n",
    "            TARGET_INDICES.append(indices)\n",
    "\n",
    "            if i == 0 :\n",
    "                TARGET_BLOCKS = target.clone().to(device)\n",
    "            else:\n",
    "                TARGET_BLOCKS = torch.cat((TARGET_BLOCKS,target),dim=1).to(device)\n",
    "\n",
    "            for i in range(len(indices)):\n",
    "                for idx in indices[i]:\n",
    "                    context_indices_i = set(context_indices[i])\n",
    "                    if idx  in context_indices_i:\n",
    "                        context_blocks[i][idx] = torch.zeros_like(context_blocks[i][idx])\n",
    "                        context_indices[i].remove(idx)\n",
    "            context_blocks, TARGET_BLOCKS = context_blocks.to(device), TARGET_BLOCKS.to(device)\n",
    "        return context_blocks, context_indices ,TARGET_BLOCKS.reshape(-1,4,64,3,16,16), TARGET_INDICES\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        patches = self.divide_into_patches(x)\n",
    "        context, context_indices = self.sample_context_blocks(patches)\n",
    "        context, context_indices,targets, target_indices  = self.remove_overlaps(context, context_indices)\n",
    "        return context, context_indices, targets, target_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GetPositionalEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GetPositionalEmbeddings, self).__init__()\n",
    "\n",
    "    def get_1d_sincos_pos_embed_from_grid(self,embed_dim, pos):\n",
    "        \"\"\"\n",
    "        embed_dim: output dimension for each position\n",
    "        pos: a list of positions to be encoded: size (M,)\n",
    "        out: (M, D)\n",
    "        \"\"\"\n",
    "        assert embed_dim % 2 == 0\n",
    "        omega = np.arange(embed_dim // 2, dtype=float)\n",
    "        omega /= embed_dim / 2.\n",
    "        omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "        pos = pos.reshape(-1)   # (M,)\n",
    "        out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "        emb_sin = np.sin(out)  # (M, D/2)\n",
    "        emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "        emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "        return emb\n",
    "\n",
    "    def get_1d_sincos_pos_embed(self,embed_dim, grid_size, cls_token=False):\n",
    "        \"\"\"\n",
    "        grid_size: int of the grid length\n",
    "        return:\n",
    "        pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "        \"\"\"\n",
    "        grid = np.arange(grid_size, dtype=float)\n",
    "        pos_embed = self.get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "        if cls_token:\n",
    "            pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "        return pos_embed\n",
    "\n",
    "\n",
    "    def get_2d_sincos_pos_embed_from_grid(self,embed_dim, grid):\n",
    "        assert embed_dim % 2 == 0\n",
    "\n",
    "        # use half of dimensions to encode grid_h\n",
    "        emb_h = self.get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "        emb_w = self.get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "        emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "        return emb\n",
    "\n",
    "    def get_2d_sincos_pos_embed(self,embed_dim, grid_size, cls_token=False):\n",
    "        \"\"\"\n",
    "        grid_size: int of the grid height and width\n",
    "        return:\n",
    "        pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "        \"\"\"\n",
    "        grid_h = np.arange(grid_size, dtype=float)\n",
    "        grid_w = np.arange(grid_size, dtype=float)\n",
    "        grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "        grid = np.stack(grid, axis=0)\n",
    "\n",
    "        grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "        pos_embed = self.get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "        if cls_token:\n",
    "            pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "        return pos_embed\n",
    "\n",
    "    def forward(self, grid_size):\n",
    "        pos_emb = self.get_2d_sincos_pos_embed(768, grid_size)\n",
    "        return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd,n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(n_embd,n_embd)\n",
    "        \n",
    "        # regularization\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        y = self.dropout(y)\n",
    "        #print(f\"this is for self : {y.shape}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd,ff_hid_dim):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(n_embd, ff_hid_dim)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(ff_hid_dim, n_embd)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "       \n",
    "        \n",
    "        return x\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head,ff_fid_dim):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = CausalSelfAttention(n_embd,n_head)\n",
    "        self.ffwd = FeedFoward(n_embd,ff_fid_dim)\n",
    "        \n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=128\n",
    "from typing import Optional\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,patchembedder,positionembedder,image_size ,patch_size, in_channels, embed_dim, num_heads,\n",
    "                 num_layers, ff_hid_dim, max_len: int = 512, target = False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_embedding = patchembedder\n",
    "        self.position_embedding = positionembedder\n",
    "        self.target = target\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads,ff_hid_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target_rep: Optional[torch.Tensor] = None, target_indices :Optional [list] = None):\n",
    "        # Step 1: Patch embedding\n",
    "        if self.target == False:\n",
    "            batch_size = x.size(0)\n",
    "            x,_,target,target_indices = self.patch_embedding(x) # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "\n",
    "            x = x.flatten(2) # (B, num_patches, embed_dim)\n",
    "\n",
    "            # Step 2: Add positional embedding\n",
    "            pos_emb = torch.from_numpy(self.position_embedding(8)).unsqueeze(dim=0).repeat_interleave(repeats = 32, dim=0).to(device)\n",
    "            pos_emb = pos_emb.float()\n",
    "            x = x +  pos_emb\n",
    "\n",
    "            # Step 3: Pass through transformer layers\n",
    "            for layer in self.encoder_layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            # Step 4: Return output for each patch\n",
    "            x, target  =x.to(device), target.to(device)\n",
    "            return x , target, target_indices # (B, num_patches, embed_dim)\n",
    "        else:\n",
    "            batch_size = x.size(0)\n",
    "            target_representations = []\n",
    "            target_blocks = target_rep.to(device)\n",
    "            for i in range(target_blocks.shape[1]):\n",
    "                x = target_blocks[:,i,:,:].flatten(2) # (B, num_patches, embed_dim)\n",
    "\n",
    "                # Step 2: Add positional embedding\n",
    "                pos_emb = torch.from_numpy(self.position_embedding(8)).unsqueeze(dim=0).repeat_interleave(repeats = 32, dim=0).to(device)\n",
    "                pos_emb = pos_emb.float()\n",
    "                x = x+  pos_emb\n",
    "\n",
    "                # Step 3: Pass through transformer layers\n",
    "                for layer in self.encoder_layers:\n",
    "                    x = layer(x)\n",
    "\n",
    "                target_representations.append(x)\n",
    "            return target_representations  # (B, num_patches, embed_dim)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_positional_embeddings = GetPositionalEmbeddings().to(device)\n",
    "patchembedder  = GetPatchEmbeddings().to(device)\n",
    "context_enocder = VisionTransformer(\n",
    "        patchembedder,\n",
    "        get_positional_embeddings,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        ff_hid_dim=768,\n",
    "        \n",
    "        target=False\n",
    "    )\n",
    "target_encoder = VisionTransformer(\n",
    "        patchembedder,\n",
    "        get_positional_embeddings,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embed_dim=768,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        ff_hid_dim=768,\n",
    "        \n",
    "        target=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer_predictor(nn.Module):\n",
    "    def __init__(self,positionembedder, patch_size, image_size, in_channels, embed_dim, num_heads,\n",
    "                 num_layers, ff_hid_dim, max_len: int = 512):\n",
    "        super(VisionTransformer_predictor, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.position_embedding = positionembedder.to(device)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads,ff_hid_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]).to(device)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 768)\n",
    "        )\n",
    "    def get_mask_embeddings(self,target_list : list):\n",
    "\n",
    "        pos_emb = torch.from_numpy(self.position_embedding(8)).unsqueeze(dim=0).repeat_interleave(repeats = 32, dim=0).to(device)\n",
    "        pos_emb = pos_emb.float()\n",
    "        for i in range(len(target_list)):\n",
    "            pos_masks = self.position_embedding.get_1d_sincos_pos_embed_from_grid(768, pos = np.array(target_list[i]))\n",
    "            pos_masks = torch.from_numpy(pos_masks).to(device)\n",
    "            pos_masks = torch.cat([pos_masks, torch.zeros(pos_emb[i].shape[0] - pos_masks.shape[0], 768).to(device)], dim=0).to(device)\n",
    "            pos_emb[i] =  pos_emb[i] + pos_masks\n",
    "        return pos_emb\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target_lists: list):\n",
    "        x = x.to(device)\n",
    "        pos_emb = self.get_mask_embeddings(target_lists)\n",
    "        pos_emb = pos_emb.float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        # Step 3: Pass through transformer layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = VisionTransformer_predictor(\n",
    "        get_positional_embeddings,\n",
    "        image_size=128,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embed_dim=768,\n",
    "        num_heads=4,\n",
    "        num_layers=6,\n",
    "        ff_hid_dim=384,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cihaz tanımlaması (GPU varsa kullanır)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_iJEPA(context_model, target_model, predictor_model, animal_dataset, num_epochs=10):\n",
    "    # Modelleri cihaza taşıyoruz\n",
    "    context_model = context_model.to(device)\n",
    "    target_model = target_model.to(device)\n",
    "    predictor_model = predictor_model.to(device)\n",
    "    \n",
    "    # DataLoader oluşturuluyor\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        animal_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Optimizatörler\n",
    "    context_opt = torch.optim.AdamW(context_model.parameters(), lr=1e-4)\n",
    "    predictor_opt = torch.optim.AdamW(predictor_model.parameters(), lr=1e-4)\n",
    "    scheduler = StepLR(predictor_opt, step_size=3, gamma=0.1)\n",
    "\n",
    "    # target_model parametrelerini donduruyoruz\n",
    "    for param in target_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Loss fonksiyonunu tanımlıyoruz: 1 - cosine similarity (ortalaması)\n",
    "    # Böylece loss her zaman 0 veya pozitif olacak.\n",
    "    criterion = lambda pred, target: (1 - F.cosine_similarity(pred, target, dim=-1)).mean()\n",
    "    \n",
    "    train_losses = []\n",
    "    context_model.train()\n",
    "    predictor_model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for idx, (x, _) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # context_model'den context, target ve target_indices elde ediliyor\n",
    "            context, target, target_indices = context_model(x)\n",
    "            \n",
    "            # target_model'den hedef temsiller (target representations) alınıyor\n",
    "            target_representations = target_model(x, target, target_indices)\n",
    "            \n",
    "            loss_accum = 0.0\n",
    "            # Her bir hedef temsil için loss hesaplanıyor\n",
    "            for i in range(len(target_representations)):\n",
    "                # predictor_model ile context temsilleri elde ediliyor.\n",
    "                # context.detach() ile geriye yayılımın target_model'e gitmesi engelleniyor.\n",
    "                context_representations = predictor_model(context.detach(), target_indices[i])\n",
    "                \n",
    "                # Loss: 1 - cosine_similarity, böylece negatif değer oluşması engelleniyor.\n",
    "                loss_i = criterion(context_representations, target_representations[i])\n",
    "                loss_accum += loss_i\n",
    "                \n",
    "            loss = loss_accum / len(target_representations)\n",
    "            \n",
    "            # Geriye yayılım\n",
    "            loss.backward()\n",
    "            \n",
    "            # Loss toplamına ekleme (skaler olarak)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Parametre güncellemesi\n",
    "            context_opt.step()\n",
    "            predictor_opt.step()\n",
    "            context_opt.zero_grad()\n",
    "            predictor_opt.zero_grad()\n",
    "            \n",
    "            # CUDA kullanıyorsak senkronizasyon\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "        \n",
    "        # Öğrenme oranı güncellemesi\n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_iJEPA(context_model, target_model, predictor_model, num_epochs = 10):\\n    context_model  = context_model.to(device)\\n    target_model = target_model.to(device)\\n    predictor_model = predictor_model.to(device)\\n    dataloader = torch.utils.data.DataLoader(animal_dataset, batch_size =32 , shuffle = True, num_workers=4,drop_last=True)\\n    \\n    context_opt= torch.optim.AdamW(context_model.parameters(), lr=1e-4)\\n    predictor_opt = torch.optim.AdamW(predictor_model.parameters(), lr=1e-4)\\n    scheduler = StepLR(predictor_opt, step_size=3, gamma=0.1)\\n\\n    for param in target_model.parameters():\\n        param.requires_grad = False\\n    train_losses = []\\n    context_model.train()\\n    predictor_model.train()\\n    train_losses = []\\n    for epochs in range(num_epochs):\\n        \\n        total_loss=0\\n        for idx,(x,_) in enumerate(tqdm(dataloader)):\\n            with torch.autograd.set_detect_anomaly(True):\\n                x = x.to(device)\\n                \\n                context, target, target_indices = context_model(x)\\n                \\n                target_representations = target_model(x,target, target_indices)\\n                loss_accum=0\\n                for i in range(len(target_representations)):\\n                    context_representations = predictor_model(context.detach(), target_indices[i])\\n                \\n               \\n                    loss_çakma = criterion(context_representations,target_representations[i])\\n                    loss_accum+=loss_çakma\\n                loss=loss_accum/len(target_representations)\\n                loss.backward()\\n                \\n                total_loss += loss\\n                context_opt.step()\\n                predictor_opt.step()\\n                context_opt.zero_grad()\\n                predictor_opt.zero_grad()\\n                if device == \"cuda\":\\n                    torch.cuda.synchronize()\\n        scheduler.step()\\n        avg_loss = total_loss / len(dataloader)\\n        train_losses.append(avg_loss)\\n       \\n        \\n        print(f\"Epoch {epochs} | Loss: {avg_loss.item():.6f}\")\\n\\n\\n\\n    print(\\'finished Training\\')\\n    return train_losses'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def train_iJEPA(context_model, target_model, predictor_model, num_epochs = 10):\n",
    "    context_model  = context_model.to(device)\n",
    "    target_model = target_model.to(device)\n",
    "    predictor_model = predictor_model.to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(animal_dataset, batch_size =32 , shuffle = True, num_workers=4,drop_last=True)\n",
    "    \n",
    "    context_opt= torch.optim.AdamW(context_model.parameters(), lr=1e-4)\n",
    "    predictor_opt = torch.optim.AdamW(predictor_model.parameters(), lr=1e-4)\n",
    "    scheduler = StepLR(predictor_opt, step_size=3, gamma=0.1)\n",
    "\n",
    "    for param in target_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    train_losses = []\n",
    "    context_model.train()\n",
    "    predictor_model.train()\n",
    "    train_losses = []\n",
    "    for epochs in range(num_epochs):\n",
    "        \n",
    "        total_loss=0\n",
    "        for idx,(x,_) in enumerate(tqdm(dataloader)):\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                x = x.to(device)\n",
    "                \n",
    "                context, target, target_indices = context_model(x)\n",
    "                \n",
    "                target_representations = target_model(x,target, target_indices)\n",
    "                loss_accum=0\n",
    "                for i in range(len(target_representations)):\n",
    "                    context_representations = predictor_model(context.detach(), target_indices[i])\n",
    "                \n",
    "               \n",
    "                    loss_çakma = criterion(context_representations,target_representations[i])\n",
    "                    loss_accum+=loss_çakma\n",
    "                loss=loss_accum/len(target_representations)\n",
    "                loss.backward()\n",
    "                \n",
    "                total_loss += loss\n",
    "                context_opt.step()\n",
    "                predictor_opt.step()\n",
    "                context_opt.zero_grad()\n",
    "                predictor_opt.zero_grad()\n",
    "                if device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "       \n",
    "        \n",
    "        print(f\"Epoch {epochs} | Loss: {avg_loss.item():.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "    print('finished Training')\n",
    "    return train_losses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "transform = transforms.Compose([\n",
    "    # Randomly flip images horizontally\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "animal_dataset = datasets.ImageFolder(root=path,  # Specify the root directory of the dataset\n",
    "                               transform=transform)  # Apply the defined transformations to the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 168/168 [01:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.087288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  15%|█▍        | 25/168 [00:20<01:59,  1.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_iJEPA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_enocder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43manimal_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 51\u001b[0m, in \u001b[0;36mtrain_iJEPA\u001b[1;34m(context_model, target_model, predictor_model, animal_dataset, num_epochs)\u001b[0m\n\u001b[0;32m     48\u001b[0m context, target, target_indices \u001b[38;5;241m=\u001b[39m context_model(x)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# target_model'den hedef temsiller (target representations) alınıyor\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m target_representations \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Her bir hedef temsil için loss hesaplanıyor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x, target_rep, target_indices)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Step 3: Pass through transformer layers\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m---> 56\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     target_representations\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_representations\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m B, T, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;66;03m# batch size, sequence length, embedding dimensionality (n_embd)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# calculate query, key, values for all heads in batch and move head forward to be the batch dim\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     25\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bahaa\\anaconda3\\envs\\altantorch2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained=train_iJEPA(context_enocder,target_encoder,predictor,animal_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altantorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
